{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashtags_data = pandas.read_csv(\"../data_collection/hashtags_data.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build feature extractor\n",
    "feature_extraction = TfidfVectorizer(min_df=0.1, sublinear_tf=True)\n",
    "feature_extraction.fit(hashtags_data[\"hashtags\"].values)\n",
    "X = feature_extraction.transform(hashtags_data[\"hashtags\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= hashtags_data[\"label\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train classifier\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC yields 1.0\n"
     ]
    }
   ],
   "source": [
    "# predict and evaluate predictions\n",
    "predictions = clf.predict(X_test)\n",
    "print('ROC-AUC yields ' + str(roc_auc_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1\n",
      " 1 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1] [0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 1 0 0 1\n",
      " 1 0 1 0 1 0 1 1 1 1 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(predictions, y_test, [x - y for x, y in zip(predictions, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Comment after first prediction\n",
    "\n",
    "The results above are great, we managed to accuratemy predict the political view for several twitter accounts. We will now look at parameter tuning, to increase our prediction score even more.\n",
    "To do so, we will use a tool provided by Scikit learn, called GridSearchCV. Given a range of possibilities for our parameters, GridSearchCV will test every combination itself, returning the best parameters for our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "0.9910714285714286\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(hashtags_data[\"hashtags\"].values)\n",
    "\n",
    "params = {\"svc__C\": [.01, .1, 1, 10, 100],\n",
    "          \"svc__kernel\": ['rbf', 'poly', 'linear'],\n",
    "          \"tfidf__min_df\": [.01, .03, .05, .1, .2],\n",
    "          \"tfidf__sublinear_tf\": [True, False]}\n",
    "\n",
    "clf = Pipeline([(\"tfidf\", TfidfVectorizer()),\n",
    "                (\"svc\", SVC())])\n",
    "\n",
    "estimator = GridSearchCV(clf, params, verbose=0, n_jobs=2)\n",
    "estimator.fit(hashtags_data[\"hashtags\"].values, hashtags_data[\"label\"].values)\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('svc', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))]\n"
     ]
    }
   ],
   "source": [
    "print(estimator.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([('vect', TfidfVectorizer(min_df=0.01, sublinear_tf=True)), ('svc', SVC(C=1, kernel='linear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our algorithm's final score is: 0.982142857143\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, hashtags_data[\"hashtags\"].values, hashtags_data[\"label\"].values, cv=3)\n",
    "print ('Our algorithm\\'s final score is:', sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test with user mentions\n",
    "\n",
    "In our data extraction we also kept the user mentions in the tweets. Let's have a quick try at using these to predict the political view, using the same techniques as hashtags. We will mostly copy the code that was already written, since the process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>stephsmithfl danp_att arringtond3 hispaniccauc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>girlscouts bradybuzz repjackyrosen averywgardi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>secretarysonny famu_1887 repallawsonjr lrobins...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>repespaillat repjohnlewis mmviverito cunydream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>officialcbc ducksunlimited agriculturede shalo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           mentions\n",
       "0      0  stephsmithfl danp_att arringtond3 hispaniccauc...\n",
       "1      0  girlscouts bradybuzz repjackyrosen averywgardi...\n",
       "2      0  secretarysonny famu_1887 repallawsonjr lrobins...\n",
       "3      0  repespaillat repjohnlewis mmviverito cunydream...\n",
       "4      0  officialcbc ducksunlimited agriculturede shalo..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions_data = pandas.read_csv(\"../data_collection/mentions_data.csv\", )\n",
    "mentions_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build feature extractor\n",
    "feature_extraction = TfidfVectorizer(min_df=0.1, sublinear_tf=True)\n",
    "feature_extraction.fit(mentions_data[\"mentions\"].values)\n",
    "X = feature_extraction.transform(mentions_data[\"mentions\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= mentions_data[\"label\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train classifier\n",
    "clf = Pipeline([('vect', TfidfVectorizer(min_df=0.05, sublinear_tf=True)), ('svm', SVC(C=1, kernel='linear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our algorithm's final score is: 0.988147914033\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, mentions_data[\"mentions\"].values, mentions_data[\"label\"].values, cv=3)\n",
    "print ('Our algorithm\\'s final score is:', sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "0.9881656804733728\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(mentions_data[\"mentions\"].values)\n",
    "\n",
    "params = {\"svc__C\": [.01, .1, 1, 10, 100],\n",
    "          \"svc__kernel\": ['linear'],\n",
    "          \"tfidf__min_df\": [.01, .03, .05, .1, .2],\n",
    "          \"tfidf__sublinear_tf\": [True]}\n",
    "\n",
    "clf = Pipeline([(\"tfidf\", TfidfVectorizer()),\n",
    "                (\"svc\", SVC())])\n",
    "\n",
    "estimator = GridSearchCV(clf, params, verbose=0, n_jobs=2)\n",
    "estimator.fit(mentions_data[\"mentions\"].values, mentions_data[\"label\"].values)\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('svc', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))]\n"
     ]
    }
   ],
   "source": [
    "print(estimator.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train classifier\n",
    "clf = Pipeline([('vect', TfidfVectorizer(min_df=0.02, sublinear_tf=True)), ('svm', SVC(C=10, kernel='linear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our algorithm's final score is: 0.997050147493\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, mentions_data[\"mentions\"].values, mentions_data[\"label\"].values, cv=3)\n",
    "print ('Our algorithm\\'s final score is:', sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick test with urls\n",
    "\n",
    "In our data extraction we also kept the urls in the tweets. Let's have a quick try at using these to predict the political view, using the same techniques as hashtags. We will mostly copy the code that was already written, since the process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://www.businessinsider.com/ http://www.tam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://lasvegassun.com/ http://www.mcclatchyd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://usat.ly/ https://medium.com/ https://m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>http://thehill.com/ https://www.healthcare.gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>http://thndr.me/ http://www.healthcare.gov/ ht...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               urls\n",
       "0      0  http://www.businessinsider.com/ http://www.tam...\n",
       "1      0  https://lasvegassun.com/ http://www.mcclatchyd...\n",
       "2      0  https://usat.ly/ https://medium.com/ https://m...\n",
       "3      0  http://thehill.com/ https://www.healthcare.gov...\n",
       "4      0  http://thndr.me/ http://www.healthcare.gov/ ht..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_data = pandas.read_csv(\"../data_collection/urls_data.csv\", )\n",
    "urls_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build feature extractor\n",
    "feature_extraction = TfidfVectorizer(min_df=0.1, sublinear_tf=True)\n",
    "feature_extraction.fit(urls_data[\"urls\"].values)\n",
    "X = feature_extraction.transform(urls_data[\"urls\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y= urls_data[\"label\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train classifier\n",
    "clf = Pipeline([('vect', TfidfVectorizer(min_df=0.05, sublinear_tf=True)), ('svm', SVC(C=1, kernel='linear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our algorithm's final score is: 0.961572903498\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, urls_data[\"urls\"].values, urls_data[\"label\"].values, cv=3)\n",
    "print ('Our algorithm\\'s final score is:', sum(scores)/len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True...,\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "0.9704142011834319\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(urls_data[\"urls\"].values)\n",
    "\n",
    "params = {\"svc__C\": [.01, .1, 1, 10, 100],\n",
    "          \"svc__kernel\": ['linear'],\n",
    "          \"tfidf__min_df\": [.01, .03, .05, .01, .02],\n",
    "          \"tfidf__sublinear_tf\": [True]}\n",
    "\n",
    "clf = Pipeline([(\"tfidf\", TfidfVectorizer()),\n",
    "                (\"svc\", SVC())])\n",
    "\n",
    "estimator = GridSearchCV(clf, params, verbose=0, n_jobs=2)\n",
    "estimator.fit(urls_data[\"urls\"].values, urls_data[\"label\"].values)\n",
    "\n",
    "print(estimator.best_estimator_)\n",
    "print(estimator.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=True,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)), ('svc', SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))]\n"
     ]
    }
   ],
   "source": [
    " print(estimator.best_estimator_.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test on supporter accounts\n",
    "\n",
    "I have gathered accounts from trump supporters. Let's see if our models manages to predict that these accounts are indeed republican accounts. The exercise is much harder than what we have done before, since these new accounts don't necessarily talk about politics a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First with mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mentions_test_data = pandas.read_csv(\"../data_collection/mentions_test_data.csv\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.02,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_mentions = Pipeline([('vect', TfidfVectorizer(min_df=0.02, sublinear_tf=True)), ('svm', SVC(C=10, kernel='linear'))])\n",
    "clf_mentions.fit(mentions_data[\"mentions\"].values, mentions_data[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC yields 0.82420212766\n"
     ]
    }
   ],
   "source": [
    "prediction_mentions = clf_mentions.predict(mentions_test_data['mentions'].values)\n",
    "print('ROC-AUC yields ' + str(roc_auc_score(mentions_test_data['label'], prediction_mentions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'échantillons tests : 87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fakenews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>reallyfakenews fbi dirtyfbi miga hypocrite pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>starwars tgif jobs gooddayny mustread bingewor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mueller specialcounsel flynn foxnews foxnews b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>readabookday deepestbluest gamerecognizegame s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           hashtags\n",
       "0      1                                           fakenews\n",
       "1      1  reallyfakenews fbi dirtyfbi miga hypocrite pre...\n",
       "2      1  starwars tgif jobs gooddayny mustread bingewor...\n",
       "3      1  mueller specialcounsel flynn foxnews foxnews b...\n",
       "4      1  readabookday deepestbluest gamerecognizegame s..."
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags_test_data = pandas.read_csv(\"../data_collection/hashtags_test_data.csv\", )\n",
    "hashtags_test_data = hashtags_test_data[hashtags_test_data['hashtags'].notnull()]\n",
    "print('Nombre d\\'échantillons tests :', hashtags_test_data.shape[0])\n",
    "hashtags_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_hashtags = Pipeline([('vect', TfidfVectorizer(min_df=0.01, sublinear_tf=True)), ('svc', SVC(C=1, kernel='linear'))])\n",
    "clf_hashtags.fit(hashtags_data[\"hashtags\"].values, hashtags_data[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC yields 0.816223404255\n"
     ]
    }
   ],
   "source": [
    "prediction_hashtags = clf_hashtags.predict(hashtags_test_data['hashtags'].values)\n",
    "print('ROC-AUC yields ' + str(roc_auc_score(hashtags_test_data['label'], prediction_hashtags)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's finish with urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'échantillons tests : 87\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>http://thehill.com/ http://thehill.com/ https:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://nypost.com/ https://www.investors.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.21cf.com/ http://press.foxbusiness...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.washingtontimes.com/ http://thehil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>http://tinyurl.com/ http://www.nydailynews.com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               urls\n",
       "0      1  http://thehill.com/ http://thehill.com/ https:...\n",
       "1      1  https://nypost.com/ https://www.investors.com/...\n",
       "2      1  https://www.21cf.com/ http://press.foxbusiness...\n",
       "3      1  https://www.washingtontimes.com/ http://thehil...\n",
       "4      1  http://tinyurl.com/ http://www.nydailynews.com..."
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_test_data = pandas.read_csv(\"../data_collection/urls_test_data.csv\", )\n",
    "urls_test_data = urls_test_data[urls_test_data['urls'].notnull()]\n",
    "print('Nombre d\\'échantillons tests :', urls_test_data.shape[0])\n",
    "urls_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=0.01,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_urls = Pipeline([('vect', TfidfVectorizer(min_df=0.01, sublinear_tf=True)), ('svc', SVC(C=10, kernel='linear'))])\n",
    "clf_urls.fit(urls_data[\"urls\"].values, urls_data[\"label\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC yields 0.626861702128\n"
     ]
    }
   ],
   "source": [
    "prediction_urls = clf_hashtags.predict(urls_test_data['urls'].values)\n",
    "print('ROC-AUC yields ' + str(roc_auc_score(urls_test_data['label'], prediction_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1] [1 1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 0] [1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n",
      " 0 1 0 0 0 1 1 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(prediction_urls, prediction_mentions, prediction_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 87 87 87\n"
     ]
    }
   ],
   "source": [
    "voting_predictions = [1 if a + b + c > 1 else 0 for a, b, c in zip(prediction_urls, prediction_mentions, prediction_hashtags)]\n",
    "print(len(voting_predictions), len(prediction_urls), len(prediction_mentions), len(prediction_hashtags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, -1, 0, -1, 0, -1, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0] -10\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0] 11\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1] 28\n",
      "[0, 0, 0, 0, -1, 0, -1, 0, -2, -1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, 0, -1, 0, 0, -1, 0, 0, -1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, -1, -1, 0, 0, 3, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 0, 0, 2, 0, 2, 0, 1, 1, 2, 1, 1, 0, 2, 2, 1, 3, 3, 1, 2, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n",
      "87\n",
      "ROC-AUC yields 0.826861702128\n"
     ]
    }
   ],
   "source": [
    "print([a-b for a,b in zip(prediction_mentions,urls_test_data['label'].values)], sum([a-b for a,b in zip(prediction_mentions,urls_test_data['label'].values)]))\n",
    "print([a-b for a,b in zip(prediction_hashtags,urls_test_data['label'].values)], sum([a-b for a,b in zip(prediction_hashtags,urls_test_data['label'].values)]))\n",
    "print([a-b for a,b in zip(prediction_urls,urls_test_data['label'].values)], sum([a-b for a,b in zip(prediction_urls,urls_test_data['label'].values)]))\n",
    "print([a + b + c - 3*d for a, b, c, d in zip(prediction_urls, prediction_mentions, prediction_hashtags, urls_test_data['label'].values)]\n",
    ")\n",
    "print([a-b for a,b in zip(voting_predictions,urls_test_data['label'].values)])\n",
    "print(len(urls_test_data['label']))\n",
    "print('ROC-AUC yields ' + str(roc_auc_score(urls_test_data['label'], voting_predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter",
   "language": "python",
   "name": "twitter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
